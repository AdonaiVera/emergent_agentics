# -*- coding: utf-8 -*-
"""Word Web.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19p-K7_YO_i3VThOi-LybNSfYpHLdRZwg
"""

import json
import re
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud
import seaborn as sns
import os
import networkx as nx
from matplotlib.colors import LinearSegmentedColormap
import pandas as pd # Added for DataFrame handling for heatmap labels
import traceback # For detailed error reporting

# Set visualization style
sns.set_style("whitegrid", {"grid.linestyle": ":", "axes.edgecolor": "0.5"})
try:
    plt.rcParams['font.family'] = 'DejaVu Sans' # Or choose another available font if needed
except Exception as font_ex:
    print(f"Warning: Could not set font 'DejaVu Sans'. Using default. Error: {font_ex}")
    # Proceed with default font

# Debug file access
print("Current directory:", os.getcwd())
# Check if the directory exists before listing files
sample_dir = 'sample_data'
if os.path.exists(sample_dir) and os.path.isdir(sample_dir):
    print(f"Files in '{sample_dir}':", ', '.join(os.listdir(sample_dir)))
else:
    print(f"Directory '{sample_dir}' not found or is not a directory.")
    # Attempt to list files in the current directory as a fallback
    print("Files in current directory:", ', '.join(os.listdir()))


# Define the path to your JSON file
# *** IMPORTANT: Make sure this path is correct for your system ***
# Example: file_path = 'data/my_conversation.json'
# Using the path from the original prompt for consistency:
file_path = 'sample_data/birthday.json'
print(f"\nAttempting to read: {file_path}")

try:
    # Read and parse file
    if not os.path.exists(file_path):
          raise FileNotFoundError(f"The specified file was not found: {file_path}")

    with open(file_path, 'r', encoding='utf-8') as file:
        file_content = file.read()

    print(f"Successfully read {len(file_content):,} characters from the file.")

    # Attempt JSON parsing
    data = None
    try:
        # Handle potential BOM (Byte Order Mark) at the start of the file
        if file_content.startswith('\ufeff'):
            file_content_cleaned = file_content.lstrip('\ufeff')
        else:
            file_content_cleaned = file_content
        data = json.loads(file_content_cleaned)
        print("JSON parsed successfully.")
    except json.JSONDecodeError as json_err:
        print(f"Could not parse JSON ({json_err}), attempting text extraction from raw content.")
        # Keep data as None, proceed with text extraction logic below
        # Use the original file_content for regex/raw extraction if JSON fails
        file_content_for_text = file_content
    else:
        # If JSON parsing succeeded, use the cleaned content (without BOM)
        file_content_for_text = file_content_cleaned


    # Text extraction logic
    all_text = ""
    # Regex to find dialogue patterns like "Speaker Name: "Dialogue text...""
    # Made speaker part non-capturing, added optional space after colon
    # Improved regex to handle potential variations and multiline quotes
    dialogue_pattern = r'(?:[\w\s.-]+:\s*")([^"]+)"'
    dialogues = re.findall(dialogue_pattern, file_content_for_text)

    if dialogues:
        print(f"Found {len(dialogues)} dialogue segments using regex.")
        all_text = ' '.join(dialogues)
    else:
        print("No dialogue segments found using regex. Attempting recursive extraction from JSON structure (if parsed) or using raw text.")
        if isinstance(data, (dict, list)): # Check if data was successfully parsed into dict or list
            # Recursive function to extract all reasonably long string values from nested structure
            def extract_text_recursive(obj):
                texts = []
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        # Optionally exclude keys or specific patterns here if needed
                        # Example: if key not in ['metadata', 'timestamps']:
                        texts.extend(extract_text_recursive(value))
                elif isinstance(obj, list):
                    for item in obj:
                        texts.extend(extract_text_recursive(item))
                elif isinstance(obj, str) and len(obj.split()) > 2: # Heuristic: extract strings with more than 2 words
                    # Basic cleaning within extracted string
                    cleaned_text = re.sub(r'[\<\(\[].*?[\>\)\]]', '', obj) # Remove bracketed content
                    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip() # Normalize whitespace
                    if cleaned_text:
                          texts.append(cleaned_text)
                return texts

            extracted_list = extract_text_recursive(data)
            if extracted_list:
                print(f"Extracted {len(extracted_list)} text segments from JSON structure.")
                all_text = ' '.join(extracted_list)
            else:
                print("Recursive extraction yielded no text, falling back to raw text cleaning.")
                # Fallback: Treat the whole file content as text, removing non-word/space chars
                all_text = re.sub(r'[^\w\s\']', ' ', file_content_for_text)
        else:
            # If not JSON dict/list and no dialogues, treat as plain text
            print("Content is not a JSON object/list and no dialogues found. Treating content as plain text.")
            all_text = re.sub(r'[^\w\s\']', ' ', file_content_for_text) # Clean raw content

    # Normalize whitespace in the final text
    all_text = re.sub(r'\s+', ' ', all_text).strip()

    if not all_text:
        raise ValueError("Failed to extract any usable text content from the file.")
    else:
        print(f"Total extracted text length (for analysis): {len(all_text):,} characters.")
        # print(f"Sample of extracted text: {all_text[:200]}...") # Uncomment for debugging


    # --- Define Stop Words ---
    # Start with a base set and add more specific ones
    # Consider using a standard library like NLTK's stopwords for a more comprehensive list
    stop_words = {
        # Standard English stop words
        'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 'to',
        'from', 'by', 'with', 'in', 'out', 'of', 'it', 'its', "it's", 'this',
        'that', 'i', 'you', 'he', 'she', 'we', 'they', 'him', 'her', 'them', 'my', 'your', 'his', 'our', 'their',
        'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',
        'had', 'do', 'does', 'did', 'will', 'would', 'shall', 'should', 'can',
        'could', 'may', 'might', 'must',
        'so', 'as', 'if', 'when', 'than', 'then', 'while', 'during', 'about', 'let',
        'how', 'why', 'which', 'who', 'whom', 'what',
        'because', 'until', 'again', 'further', 'up', 'down', 'through',

        # Common conversational fillers, discourse markers, and low-content words
        'tamara', 'taylor', 'wolfgang', 'schulz', 'ayesha', 'khan', 'maria', # Example names (customize or remove)
        'lopez', 'klaus', 'mueller',
        'really', 'like', 'good', 'great', 'lot', 'nice', 'fine', 'well',
        'yeah', 'yes', 'no', 'not', 'ok', 'okay', 'hey', 'hi', 'hello',
        'know', 'just', 'oh', 'uh', 'um', 'hmm', 'huh', 'actually',
        'get', 'got', 'go', 'going', 'went', 'make', 'made', 'say', 'said', 'see', 'saw', 'come', 'came',
        'very', 'too', 'also', 'even', 'maybe', 'quite', 'definitely', 'only', 'always', 'never',
        'here', 'there', 'now', 'some', 'any', 'all', 'other', 'another', 'such',
        'many', 'more', 'most', 'few', 'less', 'least',
        'one', 'two', 'three', # Consider context: sometimes numbers are important
        'thing', 'things', 'something', 'anything', 'nothing', 'person', 'people',
        'kind', 'sort', 'bit', 'way',
        'back', 'right', 'left', 'away',
        'mr', 'mrs', 'ms', 'dr', # Titles
        'etc', 'also', 'those', # Words specifically requested to be included in previous interactions, but generally good stop words
    }
    # You might want to refine this list based on the specific domain or topic of your text data.

    # --- Process Text ---
    print("Processing text: tokenizing, filtering stop words...")
    # Find sequences of letters, potentially including internal apostrophes (like "don't")
    words = re.findall(r'\b[a-zA-Z]+(?:\'[a-zA-Z]+)?\b', all_text.lower())

    filtered_words = [
        word for word in words
        if word not in stop_words and len(word) > 2 # Keep words longer than 2 chars
    ]

    if not filtered_words:
         # If filtering removed everything, maybe stop words are too aggressive or text was minimal
         print("Warning: No words remaining after filtering stop words and short words.")
         # Optionally, could proceed with unfiltered words or a less strict filter
         # For now, we'll raise an error if proceeding depends on filtered_words
         raise ValueError("No words left after filtering. Check text extraction and stop word list.")
    else:
         print(f"Retained {len(filtered_words):,} words after filtering (out of {len(words):,} total tokens).")

    word_counts = Counter(filtered_words)
    # Ensure top_words is calculated here for use in multiple visualizations
    top_words_list = word_counts.most_common(50) # Get overall top 50 words

    # --- Visualization 1: Styled Word Cloud ---
    print("Creating Word Cloud...")
    plt.figure(figsize=(12, 12))
    # Define colors using a list or standard colormap name
    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#3B1F2B'] # Custom color palette
    custom_cmap = LinearSegmentedColormap.from_list("custom", colors)

    # Use dict(word_counts) which handles the Counter object directly
    if word_counts: # Proceed only if there are words
        wc = WordCloud(
            width=1200,
            height=1200,
            background_color='white',
            colormap=custom_cmap, # Use the custom colormap
            max_words=100,        # Limit number of words in the cloud
            contour_width=1,      # Thinner contour line
            contour_color='#6A7A8A', # Subtler contour color
            random_state=42       # for reproducibility
        ).generate_from_frequencies(dict(word_counts)) # Pass the word counts dictionary

        plt.imshow(wc, interpolation='bilinear')
        plt.axis('off')
        plt.title('Conversation Word Cloud', fontsize=18, pad=20, color='#2E86AB')
        plt.tight_layout(pad=0)
        plt.savefig('word_cloud.png', dpi=300, bbox_inches='tight')
        # plt.show() # Comment out to prevent blocking execution if running non-interactively
        word_cloud_created = True
    else:
        print("Skipping Word Cloud: No words available to generate cloud.")
        word_cloud_created = False


    # --- Visualization 2: Semantic Network Graph (Improved Readability) ---
    print("Creating Semantic Network Graph...")
    plt.figure(figsize=(18, 14)) # Slightly larger figure size for potentially better spacing
    G = nx.Graph()
    num_network_nodes = 30 # Number of top words to include as nodes

    # --- Graph Visual Tuning Parameters ---
    graph_layout_k = 0.6        # Spring layout parameter: Higher values spread nodes more. Default is usually 1/sqrt(n).
    graph_layout_iterations = 60 # Spring layout iterations: More iterations for potentially better convergence.
    node_size_base = 100         # Minimum node size
    node_size_multiplier = 1000  # Multiplier for log-scaled frequency for node size
    edge_width_base = 0.2        # Minimum edge width
    edge_width_multiplier = 1.0  # Multiplier for log-scaled weight for edge width
    edge_alpha = 0.4             # Transparency of edges (lower is more transparent)
    label_font_size = 10         # Font size for node labels
    label_alpha = 0.9            # Transparency of labels (set below 1 if needed for overlap)
    label_bbox_alpha = 0.6       # Transparency of the label background box (0 = invisible, 1 = opaque)
    # ------------------------------------

    network_nodes_data = word_counts.most_common(num_network_nodes) # Get data based on num_network_nodes
    network_node_list = [word for word, count in network_nodes_data]

    if len(network_node_list) > 1: # Need at least 2 nodes to make a graph
        for word, count in network_nodes_data:
            # Simple grouping for color (can be replaced with community detection later if needed)
            G.add_node(word, size=count, group=hash(word) % 5)

        # Create edges based on co-occurrence within sentences
        sentences = re.split(r'[.!?]+', all_text) # Split by sentence terminators

        for sentence in sentences:
            sentence_lower = sentence.lower()
            # Find words within the sentence that are also in our network node list
            sentence_words_in_graph = [
                word for word in re.findall(r'\b[a-zA-Z]+(?:\'[a-zA-Z]+)?\b', sentence_lower) # Use same word regex
                if word in network_node_list # Check against the selected nodes
            ]

            # Connect words co-occurring within the sentence
            for i in range(len(sentence_words_in_graph)):
                for j in range(i + 1, len(sentence_words_in_graph)):
                    word1 = sentence_words_in_graph[i]
                    word2 = sentence_words_in_graph[j]
                    if word1 != word2: # Avoid self-loops
                        if G.has_edge(word1, word2):
                            G[word1][word2]['weight'] += 1
                        else:
                            G.add_edge(word1, word2, weight=1)

        # --- Graph Styling and Drawing ---
        if G.number_of_nodes() > 0:
            # Use a layout that attempts to minimize edge crossing and keeps connected nodes closer
            pos = nx.spring_layout(G, k=graph_layout_k, iterations=graph_layout_iterations, seed=42)

            # Calculate node sizes
            node_sizes = [node_size_base + np.log1p(G.nodes[node]['size']) * node_size_multiplier for node in G.nodes()]

            # Calculate edge widths (only if edges exist)
            if G.number_of_edges() > 0:
                edge_weights = [edge_width_base + np.log1p(G[u][v]['weight']) * edge_width_multiplier for u, v in G.edges()]
            else:
                edge_weights = [] # No weights if no edges

            # Define colors
            palette = sns.color_palette("viridis", 5)
            node_colors = [palette[G.nodes[node]['group']] for node in G.nodes()]

            # --- Draw the graph components ---

            # 1. Draw Edges (drawn first to be underneath nodes)
            if G.number_of_edges() > 0:
                nx.draw_networkx_edges(
                    G, pos,
                    width=edge_weights,
                    edge_color='#AAAAAA', # Lighter grey for edges
                    alpha=edge_alpha      # Apply transparency
                )
            else:
                 print("No edges found between the selected top words based on co-occurrence.")

            # 2. Draw Nodes
            nx.draw_networkx_nodes(
                G, pos,
                node_size=node_sizes,
                node_color=node_colors,
                alpha=0.95,            # Slightly transparent nodes
                edgecolors='#333333',    # Darker edge color for nodes helps definition
                linewidths=0.75        # Slightly thicker border for clarity
            )

            # 3. Draw Labels with Background Box for Readability
            label_options = {
                "font_size": label_font_size,
                "font_weight": 'normal',
                "font_color": '#1A1A1A', # Very dark grey/black
                "alpha": label_alpha,    # Label text transparency
                # Key change for readability: Add a background box
                "bbox": dict(facecolor='white',       # Box color
                             alpha=label_bbox_alpha, # Box transparency
                             edgecolor='none',        # No border for the box itself
                             boxstyle='round,pad=0.1')# Rounded box with slight padding
            }
            nx.draw_networkx_labels(G, pos, **label_options)

            # --- Final Touches ---
            plt.title(f"Semantic Network of Top {num_network_nodes} Words (Co-occurrence in Sentences)", fontsize=18, color='#2E86AB', pad=20)
            plt.axis('off') # Hide axes
            plt.tight_layout(pad=0)
            # Save with the original filename for consistency with summary
            plt.savefig('semantic_network.png', dpi=300, bbox_inches='tight')
            # plt.show() # Comment out if running non-interactively
            network_graph_created = True
            print("Improved Semantic Network Graph saved as 'semantic_network.png'") # Message reflects actual filename
        else:
            # This condition might be redundant if G.number_of_nodes() check happens earlier
            # but kept for robustness
            print("Skipping Semantic Network Graph drawing: No nodes or no edges to display.")
            network_graph_created = False
    else:
        print(f"Skipping Semantic Network Graph: Not enough top words ({len(network_node_list)}) found to build graph.")
        network_graph_created = False


    # --- Visualization 3: Frequency Bar Chart ---
    print("Creating Frequency Bar Chart...")
    plt.figure(figsize=(12, 8))
    num_bar_words = 20
    # Use the pre-calculated top_words_list slice
    top_bar_data = top_words_list[:num_bar_words]

    if top_bar_data: # Check if there are words to plot
        words_bar, counts_bar = zip(*top_bar_data)

        sns.barplot(
            x=list(counts_bar),
            y=list(words_bar),
            palette="viridis_r", # Use a reversed sequential palette
            edgecolor='grey',      # Add edge color for definition
            linewidth=0.6
        )

        plt.title(f'Top {num_bar_words} Most Frequent Words', fontsize=16, pad=15)
        plt.xlabel('Frequency Count', fontsize=12)
        plt.ylabel('') # Y-axis label is redundant with word ticks
        plt.xticks(fontsize=10)
        plt.yticks(fontsize=11) # Slightly larger font for words
        sns.despine(left=True, bottom=False) # Remove top/right spines, keep bottom
        plt.grid(axis='x', linestyle=':', linewidth=0.5, color='gray') # Add subtle vertical grid lines
        plt.tight_layout()
        plt.savefig('word_frequencies.png', dpi=300, bbox_inches='tight')
        # plt.show()
        frequency_chart_created = True
    else:
          print("Skipping Frequency Bar Chart: No words to display.")
          frequency_chart_created = False


    # --- Visualization 4: Word Co-occurrence Matrix (Heatmap with Window) --- # MODIFIED SECTION WITH DIAGNOSTICS
    print("\nCreating Word Co-occurrence Matrix (Window based)...")
    num_matrix_words = 15 # How many top words for the matrix rows/columns

    # >>> SET THE DESIRED WINDOW SIZE HERE <<<
    # >>> TRY 1, 5, 10, etc. AND COMPARE THE OUTPUT PNG FILES <<<
    adjacency_window_size = 20

    # --- Diagnostic Print: Confirming Window Size ---
    print(f"*** Using adjacency_window_size = {adjacency_window_size} for co-occurrence calculation ***")

    # Use the pre-calculated top_words_list slice
    top_matrix_words_list = [word for word, count in top_words_list[:num_matrix_words]]

    if len(top_matrix_words_list) >= 2: # Need at least 2 words for co-occurrence
        word_to_index = {word: i for i, word in enumerate(top_matrix_words_list)}
        # Initialize matrix to store counts
        cooccurrence_matrix = np.zeros((num_matrix_words, num_matrix_words), dtype=int)
        # --- Diagnostic Flag ---
        found_distant_cooccurrence = False # Flag to track if any hits beyond distance 1 occur

        print(f"Calculating co-occurrences for top {num_matrix_words} words within a window of {adjacency_window_size} words...")
        # Iterate through the sequence of filtered words
        for i in range(len(filtered_words)):
            word1 = filtered_words[i]
            # Check if the first word is one of the top words we care about
            if word1 in word_to_index:
                idx1 = word_to_index[word1] # Get its row index

                # Determine the end of the window slice, avoiding list index errors
                window_end = min(i + 1 + adjacency_window_size, len(filtered_words))

                # Check words in the window following word1
                for j in range(i + 1, window_end):
                    word_in_window = filtered_words[j]

                    # Check if the word found in the window is also one of the top words
                    if word_in_window in word_to_index:
                        idx2 = word_to_index[word_in_window] # Get its column index
                        # Increment the count for the pair (word1 -> word_in_window)
                        cooccurrence_matrix[idx1, idx2] += 1

                        # --- Diagnostic Check ---
                        distance = j - i
                        # Uncomment the line below if you want to see every single pair found and its distance
                        # print(f"  Found: '{word1}' (idx {idx1}) -> '{word_in_window}' (idx {idx2}) at distance {distance}")
                        if distance > 1:
                            found_distant_cooccurrence = True # Set flag if we find a pair beyond immediate neighbors
                        # --- End Diagnostic Check ---

        # --- Diagnostic Print: Result Summary ---
        print("Co-occurrence calculation complete.")
        if adjacency_window_size > 1: # Only relevant if window is larger than 1
            if not found_distant_cooccurrence:
                print(f"*** DIAGNOSTIC: No co-occurrences with distance > 1 found among the top {num_matrix_words} words for window size {adjacency_window_size}. ***")
                print("*** The matrix may look identical to the matrix generated with window size 1. This suggests top words in this text rarely co-occur closely beyond immediate adjacency. ***")
            else:
                print(f"*** DIAGNOSTIC: Co-occurrences with distance > 1 WERE found for window size {adjacency_window_size}. Differences compared to window size 1 should be visible. ***")
        # --- End Diagnostic Print ---

        # Create a Pandas DataFrame for easier labeling with Seaborn
        cooccurrence_df = pd.DataFrame(cooccurrence_matrix, index=top_matrix_words_list, columns=top_matrix_words_list)

        # --- Plotting ---
        plt.figure(figsize=(12, 10)) # Slightly larger figure for potentially dense matrix
        sns.heatmap(
            cooccurrence_df,
            annot=True,             # Show counts in cells
            fmt="d",                # Format counts as integers
            cmap="BuPu",            # Sequential colormap (e.g., 'Blues', 'Greens', 'BuPu')
            linewidths=.5,          # Lines between cells
            linecolor='lightgray',  # Color of the lines
            cbar=True,              # Show the color bar legend
            annot_kws={"size": 9}   # Adjust annotation font size slightly smaller
        )
        plt.title(f'Word Co-occurrence (within {adjacency_window_size} words, Top {num_matrix_words})', fontsize=14, pad=20) # Updated Title
        plt.xlabel('Word Appearing Within Window', fontsize=12) # Updated Label
        plt.ylabel('Preceding Word', fontsize=12)
        plt.xticks(rotation=45, ha='right', fontsize=10) # Rotate x-axis labels
        plt.yticks(rotation=0, fontsize=10)      # Keep y-axis labels horizontal
        plt.tight_layout(pad=1.0) # Adjust layout

        # --- Dynamic Filename for Comparison ---
        filename = f'word_cooccurrence_matrix_window_{adjacency_window_size}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"Word Co-occurrence Matrix saved as '{filename}'")
        cooccurrence_matrix_created = True
    else:
        print(f"Skipping Co-occurrence Matrix: Need at least 2 top words for matrix, found {len(top_matrix_words_list)}.")
        cooccurrence_matrix_created = False


    # --- Final Output Summary ---
    print("\n--- Analysis Complete ---")
    print("Visualizations generated:")
    if word_cloud_created:
        print(f"- Word Cloud:             {'word_cloud.png'}")
    else:
        print("- Word Cloud:             (Skipped)")

    if network_graph_created:
        print(f"- Semantic Network:       {'semantic_network.png'}")
    else:
        print("- Semantic Network:       (Skipped)")

    if frequency_chart_created:
        print(f"- Word Frequencies:       {'word_frequencies.png'}")
    else:
        print("- Word Frequencies:       (Skipped)")

    # Use the correct variable name from the modified section
    if cooccurrence_matrix_created:
        # The specific filename (including window size) is printed when saved now
        print(f"- Co-occurrence Matrix:   (Saved with window size in filename)")
    else:
        print("- Co-occurrence Matrix:   (Skipped)")
    print("-------------------------")
    # Close all plot figures if not showing them interactively
    plt.close('all')


except FileNotFoundError as fnf_error:
    print(f"\nError: {fnf_error}")
    print("Please ensure the file path is correct and the file exists in the specified location.")
except ValueError as val_error:
    # Catching the specific error raised if no text or words are found
    print(f"\nProcessing Error: {val_error}")
except Exception as e:
    # Catch any other unexpected errors
    print(f"\nAn unexpected error occurred: {type(e).__name__}: {e}")
    print("\n--- Traceback ---")
    traceback.print_exc()
    print("-----------------")

import json
import re
from collections import Counter
import numpy as np
import os
import traceback
from pathlib import Path
import colorsys
import datetime

# --- Setup (Ensure libraries are installed: pip install matplotlib numpy networkx) ---

# Attempt to import visualization libraries and set up dummy objects if they fail
try:
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    from matplotlib.colors import LinearSegmentedColormap, to_rgba # Keep to_rgba for gradient
    import networkx as nx # Import networkx here
    plt.style.use('seaborn-v0_8-whitegrid')
    mpl.rcParams['figure.dpi'] = 150
    mpl.rcParams['savefig.dpi'] = 300
    mpl.rcParams['savefig.bbox'] = 'tight'
    mpl.rcParams['savefig.pad_inches'] = 0.2
    mpl.rcParams['font.family'] = 'sans-serif'
    mpl.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']
    mpl.rcParams['axes.titleweight'] = 'bold'
    mpl.rcParams['axes.labelweight'] = 'medium'
    mpl.rcParams['axes.spines.top'] = False
    mpl.rcParams['axes.spines.right'] = False
    MATPLOTLIB_AVAILABLE = True
    NETWORKX_AVAILABLE = True
except ImportError as import_error:
    print(f"Warning: Required library not found ({import_error.name}). Visualization will be skipped.")
    MATPLOTLIB_AVAILABLE = 'matplotlib' not in str(import_error.name).lower()
    NETWORKX_AVAILABLE = 'networkx' not in str(import_error.name).lower()

    # Define dummy plt and mpl if matplotlib is not available
    if not MATPLOTLIB_AVAILABLE:
        class DummyPlt:
            def figure(*args, **kwargs): pass
            def subplots(*args, **kwargs): return None, DummyAx()
            def imshow(*args, **kwargs): pass
            def axis(*args, **kwargs): pass
            def title(*args, **kwargs): pass
            def xlabel(*args, **kwargs): pass
            def ylabel(*args, **kwargs): pass
            def xticks(*args, **kwargs): pass
            def yticks(*args, **kwargs): pass
            def xlim(*args, **kwargs): pass
            def grid(*args, **kwargs): pass
            def legend(*args, **kwargs): pass # Dummy legend call
            def tight_layout(*args, **kwargs): pass
            def savefig(*args, **kwargs): print("Skipping savefig: Matplotlib not available.")
            def close(*args, **kwargs): pass
            def gca(*args, **kwargs): return DummyGca()
            def gcf(*args, **kwargs): return DummyGcf()
            def figtext(*args, **kwargs): pass
            def Line2D(*args, **kwargs): return None

        class DummyAx: pass # Basic placeholder
        class DummyGca: pass # Basic placeholder
        class DummyGcf: pass # Basic placeholder

        plt = DummyPlt()
        mpl = type('obj', (object,), {'rcParams': {}})()
        def to_rgba(color_str, alpha=1.0): return (0, 0, 0, alpha) # Default to black, allow alpha
    else:
        # If matplotlib *is* available, but networkx isn't, still need to_rgba
        from matplotlib.colors import to_rgba

    # Define dummy nx if networkx is not available
    if not NETWORKX_AVAILABLE:
        class DummyNx:
            class Graph: pass
            def spring_layout(*args, **kwargs): return {}
            def draw_networkx_edges(*args, **kwargs): pass
            def draw_networkx_nodes(*args, **kwargs): pass
            def draw_networkx_labels(*args, **kwargs): pass
        nx = DummyNx()


# Define a modern color palette
COLORS = {
    'primary': '#396AB1',      # Blue
    'secondary': '#CC2529',    # Red
    'accent1': '#3E9651',      # Green
    'accent2': '#DA7C30',      # Orange
    'accent3': '#6B4C9A',      # Purple
    'neutral': '#535154',      # Dark gray
    'light': '#E5E5E5',        # Light gray
    'background': '#FFFFFF',   # White
    'text': '#333333'          # Near black
}


# Generate gradient colors (needed for node coloring)
def generate_color_gradient(start_color, end_color, n_colors):
    """Generate a gradient of colors between start_color and end_color."""
    try:
        start_rgb = to_rgba(start_color)[:3]
        end_rgb = to_rgba(end_color)[:3]
        start_hsv = colorsys.rgb_to_hsv(*start_rgb)
        end_hsv = colorsys.rgb_to_hsv(*end_rgb)
        colors = []
        for i in range(n_colors):
            t = i / (n_colors - 1) if n_colors > 1 else 0
            h = start_hsv[0] + t * (end_hsv[0] - start_hsv[0])
            s = start_hsv[1] + t * (end_hsv[1] - start_hsv[1])
            v = start_hsv[2] + t * (end_hsv[2] - start_hsv[2])
            rgb = colorsys.hsv_to_rgb(h, s, v)
            colors.append(rgb)
        return colors
    except Exception as e:
        # print(f"Color gradient generation failed: {e}")
        return [COLORS.get('primary', (0, 0, 1))] * n_colors


class TextVisualizer:
    def __init__(self, file_path, output_dir="output"):
        """Initialize the text visualizer with a file path."""
        self.file_path = file_path
        self.output_dir = Path(output_dir)
        self.file_content = None
        self.extracted_text = None
        self.filtered_words = None
        self.word_counts = None
        # Only track result for the semantic network
        self.results = {
            "semantic_network": False,
        }

        # Create output directory if it doesn't exist
        self.output_dir.mkdir(exist_ok=True, parents=True)

        # Define stop words (can be customized)
        self.stop_words = {
            # Standard English stop words
            'a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 'to',
            'from', 'by', 'with', 'in', 'out', 'of', 'it', 'its', "it's", 'this',
            'that', 'i', 'you', 'he', 'she', 'we', 'they', 'him', 'her', 'them', 'my', 'your', 'his', 'our', 'their',
            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',
            'had', 'do', 'does', 'did', 'will', 'would', 'shall', 'should', 'can',
            'could', 'may', 'might', 'must',
            'so', 'as', 'if', 'when', 'than', 'then', 'while', 'during', 'about', 'let',
            'how', 'why', 'which', 'who', 'whom', 'what',
            'because', 'until', 'again', 'further', 'up', 'down', 'through',

            # Common conversational fillers and discourse markers
            'really', 'like', 'good', 'great', 'lot', 'nice', 'fine', 'well',
            'yeah', 'yes', 'no', 'not', 'ok', 'okay', 'hey', 'hi', 'hello',
            'know', 'just', 'oh', 'uh', 'um', 'hmm', 'huh', 'actually',
            'get', 'got', 'go', 'going', 'went', 'make', 'made', 'say', 'said', 'see', 'saw', 'come', 'came',
            'very', 'too', 'also', 'even', 'maybe', 'quite', 'definitely', 'only', 'always', 'never',
            'here', 'there', 'now', 'some', 'any', 'all', 'other', 'another', 'such',
            'many', 'more', 'most', 'few', 'less', 'least',
            'one', 'two', 'three',
            'thing', 'things', 'something', 'anything', 'nothing', 'person', 'people',
            'kind', 'sort', 'bit', 'way',
            'back', 'right', 'left', 'away',
            'mr', 'mrs', 'ms', 'dr',
            'etc', 'also', 'those'
        }

    def read_file(self):
        """Read and handle the input file."""
        print(f"Attempting to read: {self.file_path}")
        if not os.path.exists(self.file_path):
            raise FileNotFoundError(f"The specified file was not found: {self.file_path}")
        with open(self.file_path, 'r', encoding='utf-8') as file:
            self.file_content = file.read()
        print(f"Successfully read {len(self.file_content):,} characters from the file.")
        if self.file_content.startswith('\ufeff'): # Handle BOM
            self.file_content = self.file_content.lstrip('\ufeff')
        return self.file_content

    def parse_json(self):
        """Attempt to parse the file content as JSON."""
        try:
            return json.loads(self.file_content)
        except json.JSONDecodeError as e:
            print(f"Could not parse JSON: {e}")
            return None

    def extract_text(self):
        """Extract text from the file content (JSON or plain text)."""
        all_text = ""
        data = self.parse_json()
        dialogue_pattern = r'(?:[\w\s.-]+:\s*")([^"]+)"|(?:\'[^\']*\')'
        dialogues = re.findall(dialogue_pattern, self.file_content)
        dialogues = [d for d in dialogues if d]

        if dialogues:
            print(f"Found {len(dialogues)} dialogue segments using regex.")
            all_text = ' '.join(dialogues)
        elif isinstance(data, (dict, list)):
            extracted_list = self._extract_text_recursive(data)
            if extracted_list:
                print(f"Extracted {len(extracted_list)} text segments from JSON structure.")
                all_text = ' '.join(extracted_list)
            else:
                print("Recursive extraction yielded no text, falling back to raw text cleaning.")
                all_text = re.sub(r'[^\w\s\']', ' ', self.file_content)
        else:
            print("Content is not JSON and no dialogues found via regex. Treating as plain text.")
            all_text = re.sub(r'[^\w\s\']', ' ', self.file_content)

        all_text = re.sub(r'\s+', ' ', all_text).strip()
        if not all_text:
            raise ValueError("Failed to extract any usable text content from the file.")
        self.extracted_text = all_text
        print(f"Total extracted text length: {len(self.extracted_text):,} characters.")
        return self.extracted_text

    def _extract_text_recursive(self, obj):
        """Recursively extract text from nested JSON structure."""
        texts = []
        if isinstance(obj, dict):
            for key, value in obj.items():
                if key.lower() in ['text', 'content', 'message', 'body', 'dialogue', 'speech', 'caption', 'description', 'transcript']:
                    if isinstance(value, str): texts.append(value)
                    elif isinstance(value, (dict, list)): texts.extend(self._extract_text_recursive(value))
                elif isinstance(value, (dict, list)): texts.extend(self._extract_text_recursive(value))
                elif isinstance(value, str) and len(value.split()) > 2 and len(value) < 5000:
                    cleaned_text = re.sub(r'<[^>]+>|\[.*?\]|\(.*?\)', '', value)
                    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
                    if cleaned_text: texts.append(cleaned_text)
        elif isinstance(obj, list):
            for item in obj: texts.extend(self._extract_text_recursive(item))
        elif isinstance(obj, str) and len(obj.split()) > 2 and len(obj) < 5000:
            cleaned_text = re.sub(r'<[^>]+>|\[.*?\]|\(.*?\)', '', obj)
            cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
            if cleaned_text: texts.append(cleaned_text)
        return texts

    def process_text(self):
        """Process the extracted text: tokenize and filter stop words."""
        print("Processing text: tokenizing, filtering stop words...")
        if not self.extracted_text:
            self.read_file()
            self.extract_text()
        words = re.findall(r'\b[a-zA-Z]+(?:\'[a-zA-Z]+)?\b', self.extracted_text.lower())
        self.filtered_words = [w for w in words if w not in self.stop_words and len(w) > 2]
        if not self.filtered_words:
            raise ValueError("No words left after filtering. Check text extraction and stop words.")
        print(f"Retained {len(self.filtered_words):,} words after filtering (out of {len(words):,} total).")
        self.word_counts = Counter(self.filtered_words)
        return self.filtered_words, self.word_counts

    def create_semantic_network(self, num_nodes=30, filename="semantic_network.png"):
        """Create an enhanced semantic network graph with darker and thicker edges for more connected nodes."""
        print("Creating Semantic Network Graph...")
        # Check dependencies first
        if not MATPLOTLIB_AVAILABLE or not NETWORKX_AVAILABLE:
            print("Skipping Semantic Network: 'matplotlib' or 'networkx' library not installed.")
            self.results["semantic_network"] = False
            return None

        # Ensure text is processed
        if self.word_counts is None or self.filtered_words is None:
            try:
                self.process_text()
            except ValueError as e:
                print(f"Skipping Semantic Network: Error during text processing - {e}")
                self.results["semantic_network"] = False
                return None

        if not self.word_counts:
            print("Skipping Semantic Network Graph: No words found after processing.")
            self.results["semantic_network"] = False
            return None

        plt.figure(figsize=(20, 16))
        G = nx.Graph()

        # Node coloring
        node_colors = generate_color_gradient(COLORS['primary'], COLORS['secondary'], 5)

        # Get top words
        network_nodes_data = self.word_counts.most_common(num_nodes)
        network_node_list = [word for word, _ in network_nodes_data]

        if len(network_node_list) <= 1:
            print(f"Skipping Semantic Network Graph: Not enough top words ({len(network_node_list)}) found.")
            plt.close()
            self.results["semantic_network"] = False
            return None

        # Add nodes
        for word, count in network_nodes_data:
            G.add_node(word, size=count, group=hash(word) % 5)

        # Build edges (co-occurrence within window, no self-loops)
        window_size = 5
        edge_weights = Counter()
        for i in range(len(self.filtered_words) - window_size + 1):
            window = self.filtered_words[i : i + window_size]
            words_in_window = [w for w in window if w in network_node_list]
            for j in range(len(words_in_window)):
                for k in range(j + 1, len(words_in_window)):
                    word1, word2 = words_in_window[j], words_in_window[k]
                    if word1 != word2: # Prevent self-loops
                        pair = tuple(sorted((word1, word2)))
                        edge_weights[pair] += 1

        # Add weighted edges to graph
        for (word1, word2), weight in edge_weights.items():
            if weight > 1: # Only add edges with weight > 1 for clarity
                G.add_edge(word1, word2, weight=weight)

        # Remove isolated nodes
        isolated_nodes = [node for node in G.nodes() if G.degree(node) == 0]
        G.remove_nodes_from(isolated_nodes)
        print(f"Removed {len(isolated_nodes)} isolated nodes.")

        if G.number_of_nodes() == 0:
            print("Skipping Semantic Network Graph: No connected nodes to display.")
            plt.close()
            self.results["semantic_network"] = False
            return None

        # Calculate layout
        try:
            pos = nx.spring_layout(G, k=0.8, iterations=150, seed=42, weight='weight')
        except Exception as layout_e:
            print(f"Error calculating graph layout: {layout_e}")
            plt.close()
            self.results["semantic_network"] = False
            return None

        # Calculate node sizes
        active_nodes = list(G.nodes())
        node_sizes = [100 + np.log1p(G.nodes[node]['size']) * 1200 for node in active_nodes]
        node_group_colors = [node_colors[G.nodes[node]['group']] for node in active_nodes]

        # Draw Edges with variable thickness AND opacity
        if G.number_of_edges() > 0:
            edge_weights_list = [G[u][v]['weight'] for u, v in G.edges()]
            min_raw_weight = min(edge_weights_list) if edge_weights_list else 1
            max_raw_weight = max(edge_weights_list) if edge_weights_list else 1

            # Set the range for edge width - more dramatic (3x more sensitive)
            min_edge_width = 0.5
            max_edge_width = 20.0  # Dramatically increased for more visual impact

            # Set the range for edge opacity - more dramatic (3x more sensitive)
            min_edge_alpha = 0.15  # Much lighter for weak connections
            max_edge_alpha = 0.95  # Almost fully opaque for strong connections

            # Calculate normalized weights (0-1 scale)
            if max_raw_weight > min_raw_weight:
                normalized_weights = [(weight - min_raw_weight) / (max_raw_weight - min_raw_weight)
                                        for weight in edge_weights_list]
            else:
                normalized_weights = [0.5] * len(edge_weights_list)

            # Apply power function to create non-linear, more dramatic transitions for widths
            # Higher exponent = more dramatic difference between weak and strong
            width_power = 0.33  # Use cube root for dramatic effect (1/3)

            # Calculate edge widths with non-linear scaling for more dramatic effect
            edge_widths = [min_edge_width + (norm_weight ** width_power) * (max_edge_width - min_edge_width)
                           for norm_weight in normalized_weights]

            # Also apply non-linear scaling to opacity for more dramatic effect
            alpha_power = 0.33  # Use cube root for dramatic effect (1/3)

            # Calculate edge alphas (opacity) with non-linear scaling
            edge_alphas = [min_edge_alpha + (norm_weight ** alpha_power) * (max_edge_alpha - min_edge_alpha)
                           for norm_weight in normalized_weights]

            # Generate edge colors with much more dramatic intensity changes (3x more sensitive)
            # Start with a lighter color for weak connections
            light_edge_color = '#999999'  # Light gray
            light_rgb = to_rgba(light_edge_color)[:3]
            # Very dark version for strongly connected edges
            dark_edge_color = '#000000'  # Pure black for maximum contrast
            dark_rgb = to_rgba(dark_edge_color)[:3]

            # Apply a power function to create non-linear, more dramatic transitions
            # Higher exponent = more dramatic transition between weak and strong
            power_factor = 3.0  # Cubic function for dramatic effect

            # Generate colors with dramatically varying intensity
            edge_colors = []
            for norm_weight in normalized_weights:
                # Apply power function to create non-linear transition
                enhanced_weight = norm_weight ** (1/power_factor)  # Use inverse power to make strong connections more prominent

                # Interpolate between light and dark colors based on enhanced weight
                r = light_rgb[0] + enhanced_weight * (dark_rgb[0] - light_rgb[0])
                g = light_rgb[1] + enhanced_weight * (dark_rgb[1] - light_rgb[1])
                b = light_rgb[2] + enhanced_weight * (dark_rgb[2] - light_rgb[2])
                edge_colors.append((r, g, b))

            # --- Debug Print ---
            print(f"Edge co-occurrence counts (weights): Min={min_raw_weight}, Max={max_raw_weight}")
            print(f"Calculated edge widths: Min={min(edge_widths):.2f}, Max={max(edge_widths):.2f}")
            print(f"Calculated edge alphas: Min={min(edge_alphas):.2f}, Max={max(edge_alphas):.2f}")
            # --- End Debug Print ---

            try:
                # Draw each edge individually to apply different colors and alphas
                for i, (u, v) in enumerate(G.edges()):
                    nx.draw_networkx_edges(
                        G, pos,
                        edgelist=[(u, v)],
                        width=edge_widths[i],
                        edge_color=[edge_colors[i]],
                        alpha=edge_alphas[i],
                        connectionstyle="arc3,rad=0.1"
                    )
            except Exception as draw_e:
                print(f"Error drawing network edges: {draw_e}")
        else:
            print("No edges to draw.") # Handle case with nodes but no edges

        # Draw Nodes
        try:
            nx.draw_networkx_nodes(
                G, pos, nodelist=active_nodes, node_size=node_sizes,
                node_color=node_group_colors, alpha=0.9,
                edgecolors=COLORS['neutral'], linewidths=1.5
            )
        except Exception as draw_n_e: print(f"Error drawing network nodes: {draw_n_e}")

        # Draw Labels
        label_options = {
            "font_size": 12, "font_weight": 'bold', "font_color": COLORS['text'], "alpha": 1.0,
            "bbox": {"facecolor": "white", "alpha": 0.8, "edgecolor": COLORS['light'], "boxstyle": "round,pad=0.4", "linewidth": 1},
            "horizontalalignment": 'center', "verticalalignment": 'center'
        }
        try:
            nx.draw_networkx_labels(G, pos, **label_options)
        except Exception as draw_l_e: print(f"Error drawing network labels: {draw_l_e}")

        # Add Title and Watermark
        plt.title(f"Semantic Network of Top {G.number_of_nodes()} Connected Words",
                  fontsize=24, color=COLORS['primary'], pad=20, fontweight='bold')
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
        plt.figtext(0.01, 0.01, f"Text Semantic Analysis ({timestamp})", fontsize=8, color=COLORS['light'])

        # --- LEGEND CODE REMOVED ---

        # Finalize and Save
        plt.axis('off')
        plt.tight_layout(pad=2)
        output_path = self.output_dir / filename
        try:
            plt.savefig(output_path, facecolor=COLORS['background'])
            self.results["semantic_network"] = True # Mark success
            print(f"Semantic Network saved to: {output_path}")
        except Exception as e:
            print(f"Error saving Semantic Network: {e}")
            self.results["semantic_network"] = False # Mark failure
            output_path = None
        finally:
            plt.close() # Ensure plot is closed regardless of save success

        return output_path

    def run_all(self):
        """Run the semantic network generation."""
        print("-" * 30)
        print(f"Starting Semantic Network Generation for {self.file_path}")
        print("-" * 30)
        pipeline_successful = True
        try:
            # 1. Read and Process Text
            self.read_file()
            self.extract_text()
            self.process_text() # Can raise ValueError

            # 2. Generate Semantic Network
            # Reset success flag before calling
            self.results["semantic_network"] = False
            try:
                result_path = self.create_semantic_network()
                if result_path is None:
                    # Function skipped or failed internally
                    pipeline_successful = False
            except Exception as viz_e:
                print(f"\n--- Error during semantic network execution ---")
                print(f"Error: {viz_e}")
                traceback.print_exc()
                self.results["semantic_network"] = False # Ensure marked failed
                pipeline_successful = False

        except FileNotFoundError as e:
            print(f"\n--- ERROR: File not found ---")
            print(e)
            pipeline_successful = False
            self.results["semantic_network"] = False
        except ValueError as e: # Catch errors during text processing
            print(f"\n--- ERROR: Data processing issue ---")
            print(e)
            pipeline_successful = False
            self.results["semantic_network"] = False
        except Exception as e: # Catch any other unexpected errors
            print(f"\n--- An unexpected error occurred during the main pipeline ---")
            print(f"Error: {e}")
            traceback.print_exc()
            pipeline_successful = False
            self.results["semantic_network"] = False

        print("-" * 30)
        print("Semantic Network Generation Finished")
        status = 'Success' if self.results["semantic_network"] else 'Failed or Skipped (check logs/dependencies)'
        print(f"- Semantic Network: {status}")
        print("-" * 30)

        if not pipeline_successful:
                print("NOTE: The process encountered errors or was skipped.")

        return self.results

# Example Usage
if __name__ == "__main__":

    # Define the input file path and output directory
    input_file = "sample_data/informal.json"
    output_dir_main = "output_costume_movie_party"

    # --- Check if the input file exists ---
    if not os.path.exists(input_file):
        print(f"ERROR: Input file not found: {input_file}")
        print("Please ensure the file exists or create a placeholder.")
        # Example placeholder creation (optional)
        # Path("sample_data").mkdir(exist_ok=True)
        # try:
        #     with open(input_file, "w", encoding="utf-8") as f:
        #         json.dump([{"text": "placeholder text example one"}], f)
        #     print(f"Created placeholder file at {input_file}")
        # except Exception as create_e:
        #     print(f"Failed to create placeholder file: {create_e}")

    # --- Run the visualizer ---
    print("\n" + "="*5 + f" Running on {input_file} " + "="*5)
    if os.path.exists(input_file):
        visualizer_main = TextVisualizer(input_file, output_dir=output_dir_main)
        results_main = visualizer_main.run_all()
    else:
        print(f"\nSkipping visualization: Input file not found.")

    print(f"\nProcess complete. Check the '{output_dir_main}' folder for the semantic network image (if generated).")
    print("NOTE: Ensure matplotlib and networkx are installed (`pip install matplotlib networkx numpy`).")

